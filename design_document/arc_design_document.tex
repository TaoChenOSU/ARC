\documentclass[compsoc,draftclsnofoot,onecolumn,10pt]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{url}

\usepackage{enumitem}

\usepackage[letterpaper, margin=.75in]{geometry}

\usepackage{hyperref}
\usepackage{listings}

\usepackage[dvipsnames]{xcolor}
\usepackage{pgfgantt}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\renewcommand{\lstlistingname}{Code Example} % a listing caption title.

\lstset{
	frame=single,
	language=C,
	columns=flexible,
	numbers=left,
	numbersep=5pt,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=4,
	captionpos=b
}

\def\name{Cierra Shawe, Daniel Stoyer, Tao Chen}

%% The following metadata will show up in the PDF properties
\hypersetup{
	colorlinks = false,
	urlcolor = black,
	pdfauthor = {\name},
	pdfkeywords = {Design Document, Fall 2016},
	pdftitle = {Design Document},
	pdfsubject = {Design Document for ARCt},
	pdfpagemode = UseNone
}

\def\myversion{1.0 }
\date{}
%
%\usepackage{titlesec}
%
%\usepackage{hyperref}

\parindent = 0.0 in
\parskip = 0.1 in

\begin{document}

\begin{titlepage}
	\title{Design Document\\
	ARC - Autonomous RC}
	\author{Tao Chen, Cierra Shawe, Daniel Stoyer}
	\maketitle
	\begin{center}
	Version 1.0\\
	\vspace{1.9cm}
	\today
	\end{center}

	\thispagestyle{empty} % gets rid of the "0" page number.
	
\end{titlepage}

\tableofcontents

\newpage

% What
Vision systems are used to det
% Design
% How it's implemented 

\section{Overview}  
\subsection{Scope} 
\subsection{Purpose}
\subsection{Intended Audience} 
\subsection{Conformance}

\section{Definitions} 

\section{Vision System}
ARCS uses a stereoscopic vision system to aid in navigation and obstacle avoidance. 
Vision systems are the "eyes" of the vehicle and are what allow the vehicle to operate autonomously. 
The vision system relates to requirement FR-IA1 and is a precursor to requirements FR-IA2 and FR-IA3. 
FR-IA1 is relevant because images must be captured and processed at a rate of 15 frames per second or higher to allow the vehicle to navigate quickly without colliding with objects.
The vision system uses input from two USB cameras to create a depth map to detect objects at least 8" high to avoid collisions with objects that the vehicle is unable to drive over. 
OpenCV and ROS are used to process images from two different USB cameras, plugged into the MCU.  
ROS handles depth maps using a SLAM library in either a point cloud or occupancy grid map format, and the output is required for FR-NAV6. \par
The first step to testing whether the vision system works is ensuring input is received from both cameras simultaneously through visual observation on a display. Then after input is successfully received, the cameras must be calibrated using OpenCV. The calibration process involves using a checkerboard and taking images from various locations. The MCU plugged into a display, must then display a depth map based on the disparity between the left and right images. 
Once a depth map is created, frame rate must be compared to find the optimal resolution to process images successfully at 15 frames a second, with 60 frames being an ideal number. Frames per second are outputted onto a display for the user to see in order to ensure the frame-rate is above the target. \par
Currently, the optimal distance between cameras is not known, so testing needs to be done to see how far apart the cameras must be placed to view objects between two and 20 feet away. The optimal range would be 6 inches to 100 feet of range. However, this goal is unlikely due to the restraints in image quality and the ability of the MCU to process the disparity maps in real-time. Using lower quality images provides fewer data points for OpenCV to compare, with a tendency towards unclear pixels on the edges of the images, unclear images when objects are very close to the cameras, and poor depth calculation when pixels are too far apart. \par
Other sensors, such as sonar or LiDAR, can be used alongside the stereo-vision cameras if the stereo cameras are unable to process objects within three feet of the vehicle. Other vision sensors are only used when it is determined stereo-vision alone is not enough to detect obstacles at close range. \par
Testing for the optimal distance for the two USB cameras is done by calibrating the cameras and holding a white piece of paper in front of the cameras from different distances, and then determining when the cameras can no longer detect the depth. 
After the camera distance have been calculated, the two cameras are mounted either via a 3d printed case or integrated near the car's headlights. 


\section{System Interfaces}
The main computational unit (MCU), an Intel NUC SkullCanyon, is used as a starting point to test the feasibility of stereo-vision for a vision system as per FR-IA1. The NUC runs Ubuntu 14.04 to provide a graphical user interface for observing inputs such as the vision system and sensor input. Ubuntu 14.04 is also a standard used to run ROS and handle input from other nodes. 
All of the data processing will be done on the MCU. \par
The MCU interfaces in the following ways:
\begin{itemize}
\item Telemetry radio via a USB connection
\item Raspberry Pi 2 via an ethernet connection
\item Vision system via two USB ports. 
\end{itemize}
19V of power is the manufacturer's rated usage during peak performance for the Intel NUC. Testing must be done to see if the NUC can run on 12V power, which is the RC battery standard, while running complex computations such as processing images into a point cloud. \\
The NUC should be able to process at least 15 frames a second without powering off while using 12V power. 
If the NUC is unable to run on 12V power, a new method for powering the NUC is needed, or a less powerful board, such as the UPBoard or a NVIDIA Jetson must be used. Using a less powerful board limits the speed in which vision is processed.  \par

The Raspberry Pi interfaces with the PXFmini autopilot through a 40pin connection. Power is supplied via a micro USB port, or through the PXFmini, which has the option to be powered through a pass-through connection to main LiPo battery. The Raspberry Pi runs Debian Jessie ("cyan"), which is provided by Erle Robotics and is designed to process input from the PXFmini. The Raspberry Pi and PXFmini control motors and sensors, and collect sensor data. \par

Data is passed via a client-server model to the MCU using a TCP connection over an Ethernet connection. Packets are sent as JSON if the Raspberry Pi cannot be configured as a node within ROS. \par

The PXFmini provides sensor data and can directly interface through UART and I2C connections with a ublox Neo GPS with Compass unit and provides built-in support. The GPS unit can also interface through an IMU for more complex and accurate motion information. An IMU unit requires adapters to interface with the new Drone Foundation standard. 
Servos and a motor speed controller functions interface through the pulse width modulation pins on the PXFmini. The PXFmini abstracts the majority of motor and servo functions. \par

The system will be tested in incremental steps to ensure that all of the devices are talking to each other. 
The first step is to establish a connection between the MCU and the Raspberry Pi via TCP. Then a ping packet will be sent, with an expected response of pong. \par
With a working connection, the first goal is to control the direction of a motor from a command sent from ROS to the PXFmini. The motor should be able to spin one way when told to go forward, and reverse when a  reverse command is sent. Then servo control will be tested. \par
After motor and servo control is established, sensor data will be collected and sent to the MCU. Two servos should be able to run synchronously to simulate steering. Data should be able to be displayed on to the user through the ROS interface. 
At this point, the system can be interfaced into the car with extreme caution. Once the servos and speed controller from the car has been connected to the PXFmini, all functions should be tested on minimum speed for the case of motors or the using least amount of travel for servos, to ensure all actions are as expected. \par

\section{Image Analysis}
Author: Dan Stoyer\par
The ROS image\_transport (http://wiki.ros.org/image\_transport) library should
be used to pass images from the sensors into rtabmap\_ros i. rtabmap\_ros. The
rtabmap\_ros library handles depth-finding and environment mapping. 

Milestone #1: Detect an object
Use ROS image\_transport (http://wiki.ros.org/image\_transport) to pass images
into rtabmap\_ros (http://wiki.ros.org/rtabmap\_ros). rtabmap\_ros analyzes
the images to detect objects. ~proj\_min\_cluster\_size (int, default: 20) is
used to control the size of object detected.

Milestone #2: Detect an object while moving
After determining that rtabmap\_ros is detecting an object while the vehicle is
stationary, object detection should be tested while moving.

Milestone #4: Depth finding
Once objects are detected, the distance to the object should be found by
passing the images to rtabmap\_ros's left/image\_rect (sensor\_msgs/Image) and
right/image\_rect (sensor\_msgs/Image). The depth analyzed from the images
should be tracked using ~subscribe\_depth (bool, default: "false").

Milestone #5: Hand off analysis
The object detection and depth information should be handed over to the
path-finding algorithm.

\section{User Interface}
Author: Dan Stoyer\par
The user interface (UI) allows the user to issue commands and see the state of
the vehicle. There are two UI platforms at play: the ground station and the companion computer.
The companion computer UI is the most important to get working right away.

Milestone #1: CLI for companion computer.
The robotics operating system (ROS) provides CLI for vehicle control. This
interface is on the companion computer only.

Milestone #2: See vehicle status on remote terminal.
The companion computer's UI should be transmitted over wifi through screen-share
software. This approach allows immediate viewing of what is happening on the
companion computer which allows time to work on more advanced remote UI solutions.

Milestone #3: Enter CLI commands on the ground station.
The simplest place to start with remote commands is from the command line.
The ground station will be running either Linux or Windows. In the case of
Windows, a third-party terminal (such as MinGW or Cygwin) should be used.

Milestone #4: See vehicle status via ground station GUI.
To see the vehicle status, a GUI is needed on the ground station. The ground
station will run either a Linux distribution or Windows 7,8,10. ArduPilot runs
on both and should be the first GUI tested. If ArduPilot does not 
Telemetry data should be sent with MAVLink protocol. Image data should be sent
to rtabmap\_ros using image\_transport (http://wiki.ros.org/image\_transport).

\section{Radio Communication}
Author: Dan Stoyer\par

Milestone #1: Send/Receive telemetry data
The companion computer should send telemetry data by radio using the MAVLink
protocol. The ground station should receive the telemetry data by radio using
the MAVLink protocol.

Milesone #2: Send commands
The ground station should send commands. Testing for seding commands should
start with simple "start" and "stop" commands and gradually increase in
complexity up to enter waypoints for autonomous navigation.


\section{Appendix}
\subsection{Annex A  Bibliography} 
% References
\bibliographystyle{IEEEtran}
\bibliography{design}


\end{document}
