\documentclass[compsoc,draftclsnofoot,onecolumn,10pt]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{url}

\usepackage{enumitem}

\usepackage[letterpaper, margin=.75in]{geometry}

\usepackage{hyperref}
\usepackage{listings}

\usepackage[dvipsnames]{xcolor}
\usepackage{pgfgantt}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\renewcommand{\lstlistingname}{Code Example} % a listing caption title.

\lstset{
	frame=single,
	language=C,
	columns=flexible,
	numbers=left,
	numbersep=5pt,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=4,
	captionpos=b
}

\def\name{Cierra Shawe, Daniel Stoyer, Tao Chen}

%% The following metadata will show up in the PDF properties
\hypersetup{
	colorlinks = false,
	urlcolor = black,
	pdfauthor = {\name},
	pdfkeywords = {Design Document, Fall 2016},
	pdftitle = {Design Document},
	pdfsubject = {Design Document for ARCt},
	pdfpagemode = UseNone
}

\def\myversion{1.0 }
\date{}
%
%\usepackage{titlesec}
%
%\usepackage{hyperref}

\parindent = 0.0 in
\parskip = 0.1 in

\begin{document}

\begin{titlepage}
	\title{Design Document\\
	ARC - Autonomous RC}
	\author{Tao Chen, Cierra Shawe, Daniel Stoyer}
	\maketitle
	\begin{center}
	Version 1.0\\
	\vspace{1.9cm}
	\today
	\end{center}

	\thispagestyle{empty} % gets rid of the "0" page number.
	
\end{titlepage}

\tableofcontents

\newpage

% What
Vision systems are used to det
% Design
% How it's implemented 

\section{Overview}  
\subsection{Scope} 
\subsection{Purpose}
\subsection{Intended Audience} 
\subsection{Conformance}

\section{Definitions} 

\section{Vision System}
ARCS uses a stereoscopic vision system to aid in navigation and obstacle avoidance. 
Vision systems are the "eyes" of the vehicle and are what allow the vehicle to operate autonomously. 
The vision system relates to requirement FR-IA1 and is a precursor to requirements FR-IA2 and FR-IA3. 
FR-IA1 is relevant because images must be captured and processed at a rate of 15 frames per second or higher to allow the vehicle to navigate quickly without colliding with objects.
The vision system uses input from two USB cameras to create a depth map to detect objects at least 8" high to avoid collisions with objects that the vehicle is unable to drive over. 
OpenCV and ROS are used to process images from two different USB cameras, plugged into the MCU.  
ROS handles depth maps using a SLAM library in either a point cloud or occupancy grid map format, and the output is required for FR-NAV6. \par
The first step to testing whether the vision system works is ensuring input is received from both cameras simultaneously through visual observation on a display. Then after input is successfully received, the cameras must be calibrated using OpenCV. The calibration process involves using a checkerboard and taking images from various locations. The MCU plugged into a display, must then display a depth map based on the disparity between the left and right images. 
Once a depth map is created, frame rate must be compared to find the optimal resolution to process images successfully at 15 frames a second, with 60 frames being an ideal number. Frames per second are outputted onto a display for the user to see in order to ensure the frame-rate is above the target. \par
Currently, the optimal distance between cameras is not known, so testing needs to be done to see how far apart the cameras must be placed to view objects between two and 20 feet away. The optimal range would be 6 inches to 100 feet of range. However, this goal is unlikely due to the restraints in image quality and the ability of the MCU to process the disparity maps in real-time. Using lower quality images provides fewer data points for OpenCV to compare, with a tendency towards unclear pixels on the edges of the images, unclear images when objects are very close to the cameras, and poor depth calculation when pixels are too far apart. 
Other sensors, such as sonar or LiDAR, can be used alongside the stereo-vision cameras if the stereo cameras are unable to process objects within three feet of the vehicle. Other vision sensors are only used when it is determined stereo-vision alone is not enough to detect obstacles at close range. 
Testing for the optimal distance for the two USB cameras is done by calibrating the cameras and holding a white piece of paper in front of the cameras from different distances, and then determining when the cameras can no longer detect the depth. 
After the camera distance have been calculated, the two cameras are mounted either via a 3d printed case or integrated near the car's headlights. 


\section{Conceptual model for software design descriptions} 
\subsection{Software design in context}
\subsection{Software design descriptions within the life cycle}

\section{Design description information content} 
\subsection{Introduction}
\subsection{SDD identification}
\subsection{Design stakeholders and their concerns} 
\subsection{Design views}
\subsection{Design viewpoints} 
\subsection{Design elements} 
\subsection{Design overlays}
\subsection{Design rationale}
\subsection{Design languages}

\section{Design viewpoints}
\subsection{Introduction} 
\subsection{Context viewpoint} 
\subsection{Composition viewpoint} 
\subsection{Logical viewpoint} 
\subsection{Dependency viewpoint} 
\subsection{Information viewpoint} 
\subsection{Patterns use viewpoint} 
\subsection{Interface viewpoint} 
\subsection{Structure viewpoint} 
\subsection{Interaction viewpoint} 
\subsection{State dynamics viewpoint} 
\subsection{Algorithm viewpoint} 
\subsection{Resource viewpoint}

\section{Appendix}
\subsection{Annex A  Bibliography} 
\subsection{Annex B  Conforming design language description} 
\subsection{Annex C Templates for an SDD}

\end{document}
