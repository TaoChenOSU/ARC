\documentclass[compsoc,draftclsnofoot,onecolumn,10pt]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{url}

\usepackage{enumitem}

\usepackage[letterpaper, margin=.75in]{geometry}

\usepackage{hyperref}
\usepackage{listings}

\usepackage[dvipsnames]{xcolor}
\usepackage{pgfgantt}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\renewcommand{\lstlistingname}{Code Example} % a listing caption title.

\lstset{
	frame=single,
	language=C,
	columns=flexible,
	numbers=left,
	numbersep=5pt,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=4,
	captionpos=b
}

\def\name{Cierra Shawe, Daniel Stoyer, Tao Chen}

%% The following metadata will show up in the PDF properties
\hypersetup{
	colorlinks = false,
	urlcolor = black,
	pdfauthor = {\name},
	pdfkeywords = {Design Document, Fall 2016},
	pdftitle = {Design Document},
	pdfsubject = {Design Document for ARCt},
	pdfpagemode = UseNone
}

\def\myversion{1.0 }
\date{}
%
%\usepackage{titlesec}
%
%\usepackage{hyperref}

\parindent = 0.0 in
\parskip = 0.1 in

\begin{document}

\begin{titlepage}
	\title{Design Document\\
	ARC - Autonomous RC}
	\author{Tao Chen, Cierra Shawe, Daniel Stoyer}
	\maketitle
	\begin{center}
	Version 1.0\\
	\vspace{1.9cm}
	\today
	\end{center}

	\thispagestyle{empty} % gets rid of the "0" page number.
	
\end{titlepage}

\tableofcontents

\newpage

% What
Vision systems are used to det
% Design
% How it's implemented 

\section{Overview}  
\subsection{Scope} 
\subsection{Purpose}
\subsection{Intended Audience} 
\subsection{Conformance}

\section{Definitions} 

\section{Vision System - Cierra}
ARCS uses a stereoscopic vision system to aid in navigation and obstacle avoidance. 
Vision systems are the "eyes" of the vehicle and are what allow the vehicle to operate autonomously. 
The vision system relates to requirement FR-IA1 and is a precursor to requirements FR-IA2 and FR-IA3. 
FR-IA1 is relevant because images must be captured and processed at a rate of 15 frames per second or higher to allow the vehicle to navigate quickly without colliding with objects.
The vision system uses input from two USB cameras to create a depth map to detect objects at least 8" high to avoid collisions with objects that the vehicle is unable to drive over. 
OpenCV and ROS are used to process images from two different USB cameras, plugged into the MCU.  
ROS handles depth maps using a SLAM library in either a point cloud or occupancy grid map format, and the output is required for FR-NAV6. \par
The first step to testing whether the vision system works is ensuring input is received from both cameras simultaneously through visual observation on a display. Then after input is successfully received, the cameras must be calibrated using OpenCV. The calibration process involves using a checkerboard and taking images from various locations. The MCU plugged into a display, must then display a depth map based on the disparity between the left and right images. 
Once a depth map is created, frame rate must be compared to find the optimal resolution to process images successfully at 15 frames a second, with 60 frames being an ideal number. Frames per second are outputted onto a display for the user to see in order to ensure the frame-rate is above the target. \par
Currently, the optimal distance between cameras is not known, so testing needs to be done to see how far apart the cameras must be placed to view objects between two and 20 feet away. The optimal range would be 6 inches to 100 feet of range. However, this goal is unlikely due to the restraints in image quality and the ability of the MCU to process the disparity maps in real-time. Using lower quality images provides fewer data points for OpenCV to compare, with a tendency towards unclear pixels on the edges of the images, unclear images when objects are very close to the cameras, and poor depth calculation when pixels are too far apart. \par
Other sensors, such as sonar or LiDAR, can be used alongside the stereo-vision cameras if the stereo cameras are unable to process objects within three feet of the vehicle. Other vision sensors are only used when it is determined stereo-vision alone is not enough to detect obstacles at close range. \par
Testing for the optimal distance for the two USB cameras is done by calibrating the cameras and holding a white piece of paper in front of the cameras from different distances, and then determining when the cameras can no longer detect the depth. 
After the camera distance have been calculated, the two cameras are mounted either via a 3d printed case or integrated near the car's headlights. 


\section{System Interfaces - Cierra}
The main computational unit (MCU), an Intel NUC SkullCanyon, is used as a starting point to test the feasibility of stereo-vision for a vision system as per FR-IA1. The NUC runs Ubuntu 14.04 to provide a graphical user interface for observing inputs such as the vision system and sensor input. Ubuntu 14.04 is also a standard used to run ROS and handle input from other nodes. 
All of the data processing will be done on the MCU. \par
The MCU interfaces in the following ways:
\begin{itemize}
\item Telemetry radio via a USB connection
\item Raspberry Pi 2 via an ethernet connection
\item Vision system via two USB ports. 
\end{itemize}
19V of power is the manufacturer's rated usage during peak performance for the Intel NUC. Testing must be done to see if the NUC can run on 12V power, which is the RC battery standard, while running complex computations such as processing images into a point cloud. \\
The NUC should be able to process at least 15 frames a second without powering off while using 12V power. 
If the NUC is unable to run on 12V power, a new method for powering the NUC is needed, or a less powerful board, such as the UPBoard or a NVIDIA Jetson must be used. Using a less powerful board limits the speed in which vision is processed.  \par

The Raspberry Pi interfaces with the PXFmini autopilot through a 40pin connection. Power is supplied via a micro USB port, or through the PXFmini, which has the option to be powered through a pass-through connection to main LiPo battery. The Raspberry Pi runs Debian Jessie ("cyan"), which is provided by Erl Robotics and is designed to process input from the PXFmini. The Raspberry Pi and PXFmini control motors and sensors, and collect sensor data. \par

Data is passed via a client-server model to the MCU using a TCP connection over an Ethernet connection. Packets are sent as JSON if the Raspberry Pi cannot be configured as a node within ROS. \par

The PXFmini provides sensor data and can directly interface through UART and I2C connections with a ublox Neo GPS with Compass unit and provides built-in support. The GPS unit can also interface through an IMU for more complex and accurate motion information. An IMU unit requires adapters to interface with the new Drone Foundation standard. 
Servos and a motor speed controller functions interface through the pulse width modulation pins on the PXFmini. The PXFmini abstracts the majority of motor and servo functions. \par

The system will be tested in incremental steps to ensure that all of the devices are talking to each other. 
The first step is to establish a connection between the MCU and the Raspberry Pi via TCP. Then a ping packet will be sent, with an expected response of pong. \par
With a working connection, the first goal is to control the direction of a motor from a command sent from ROS to the PXFmini. The motor should be able to spin one way when told to go forward, and reverse when a  reverse command is sent. Then servo control will be tested. \par
After motor and servo control is established, sensor data will be collected and sent to the MCU. Two servos should be able to run synchronously to simulate steering. Data should be able to be displayed on to the user through the ROS interface. 
At this point, the system can be interfaced into the car with extreme caution. Once the servos and speed controller from the car has been connected to the PXFmini, all functions should be tested on minimum speed for the case of motors or the using least amount of travel for servos, to ensure all actions are as expected. \par

\section{Sensors - Cierra}
Sensors provide critical information that is used to provide feedback for where the vehicle is within a space. Sensors are a pre-requisite for all navigation criteria. FR-SN1, FR-SN2, FR-SN3 are the requirements for all sensor input. 
Sensors required for the vehicle are the following:
\begin{itemize}
\item Accelerometer to calculate acceleration and deceleration 
\item Gyroscope for orientation
\item GPS with a compass for location
\item Barometer for altitude 
\item Ultrasonic sensors for parallel parking and forward collision avoidance
\item Encoders to measure wheel rotations (as needed)
\end{itemize}

The accelerometer and gyroscope are included on the PXFmini as a 9-axis sensor, or the data can be pulled from the external IMU.  
The GPS unit can be plugged directly into the PXFmini using one of the UART and I2C ports. 
Barometer data is not strictly required. However, the PXFmini has a built-in sensor, so it can be used as needed.\par

Sensor data is sent through the Raspberry Pi to the MCU, where it is processed by path planning and control algorithms. Obtaining sensor data is required before these algorithms are able to be implemented. \par

An I2C port will be multiplexed to support additional sensors if needed. 
Each sensor will be assigned a UID to be accessed through the multiplexed port. \par

Testing sensor output will be done by displaying values to the user to ensure the sensors are sending information to the MCU. The 9-axis sensor will be placed on a level sensor to ensure that it is properly calibrated. \par

\section{Hardware - Cierra}
All hardware components will be tied down and encased with the minimal amount of port access required for the car to function as per FR-HW1. 3D printing and laser cut cases will be used, and venting will be used as necessary. This is to protect the hardware from rollovers as per FR-HW2. \par
Hardware layout will first be done on cardboard to ensure components are in the correct placement on the board. 
After the layout is determined, cases for objects will be modeled and printed, then secured onto an acrylic or polycarbonate laser-cut plate. \par


\section{Conceptual model for software design descriptions} 
\subsection{Software design in context}
\subsection{Software design descriptions within the life cycle}

\section{Design description information content} 
\subsection{Introduction}
\subsection{SDD identification}
\subsection{Design stakeholders and their concerns} 
\subsection{Design views}
\subsection{Design viewpoints} 
\subsection{Design elements} 
\subsection{Design overlays}
\subsection{Design rationale}
\subsection{Design languages}

\section{Design viewpoints}
\subsection{Introduction} 
\subsection{Context viewpoint} 
\subsection{Composition viewpoint} 
\subsection{Logical viewpoint} 
\subsection{Dependency viewpoint} 
\subsection{Information viewpoint} 
\subsection{Patterns use viewpoint} 
\subsection{Interface viewpoint} 
\subsection{Structure viewpoint} 
\subsection{Interaction viewpoint} 
\subsection{State dynamics viewpoint} 
\subsection{Algorithm viewpoint} 
\subsection{Resource viewpoint}

\section{Appendix}
\subsection{Annex A  Bibliography} 
% References
\bibliographystyle{IEEEtran}
\bibliography{design}

\subsection{Annex B  Conforming design language description} 
\subsection{Annex C Templates for an SDD}

\end{document}
